---
title: "Stats 402 ~ Homework 1"
author: "Britney Brown, Yuandong(David) Sun, Jaehee Jeong, Tianyang Liu, Jeremy Weidner, Harrison Distefano, Lisa Kaunitz"
date: "10/19/2020"
output: pdf_document
---

# Problem One
Using the following campus climate data set (data folder week one), answer the following questions: 
```{r}
campusclimate <- read.csv("campusclimate.csv")
attach(campusclimate)
```

### a)	Create a scatterplot for showing UCLA students’ perception of friendliness on our campus (friendlyenvp), predictor, as a function of their perception of academic satisfaction on our campus (academicenvp), outcome. 
```{r}
#there is a slight positive trend
plot(academicenvp ~ friendlyenvp, 
     main = "Scatterplot: Perception of Friendliness Vs. Academic Satisfaction")
```

### b)	Now create a scatterplot like the one shown on (same plot as page 21 of lecture one). Remember to install the “car” package. Explain what the lines in this plot show? What do you conclude from the two boxplots? 
```{r}
#install.packages("car")
library(car)
scatterplot(academicenvp ~ friendlyenvp,
            main = "Scatterplot: Perception of Friendliness Vs. Academic Satisfaction")
```
The straight line is the least square regression line where residuals are minimized while the dashed curved line is a non-parametric lowess line that passes through the average y value for any given x value (the upper and lower broken lines smooth the positive and negative residuals from the lowess line). While both lines model closely to the right of the graph, they vary greatly on the low outliers.


The boxplots show the conditional distributions for the perception of friendliness and the perception of academic satisfaction. The horizontal boxplot represents the relative frequency of the academic satisfaction variable given a specific value of friendliness while the vertical boxplot represents the relative frequency of the friendliness variable given a specific value of academic satisfaction. Like the difference between the least square regression and lowess lines, these boxplots show the variation and the outliers on the lower end of each variable.

### c)	As you see from the two plots you created in parts a and b, there is very little data below ** ***friendlyenvp = 40*** **and below** ***academicenvp = 10.*** **Create a new subset by deleting the above range of data, attach this new data set and draw the scatter plot you drew in part b. Use the following reference as guideline for sub-setting your data. I have also included the hsb2 data in the data folder of week one so you can recreate what you find in the following reference. Comment how everything changed. https://stats.idre.ucla.edu/r/modules/subsetting-data/ 
```{r}
newdata <- campusclimate[academicenvp > 10 & friendlyenvp > 40,] #subset original dataset
attach(newdata)
scatterplot(academicenvp ~ friendlyenvp,
            main = "Scatterplot: Perception of Friendliness Vs. Academic Satisfaction")
```
With the new subset of data, the least square regression line appears closer to the lowess line throughout the graph, no longer varying greatly near the lower end of the variables. We also see a lot less outliers in the horizontal boxplot which represents the conditional distribution of academic satisfaction given friendliness. This means that this subset should be able to create a more accurate model than the original dataset.

### d)	Now create two linear models for the prediction of UCLA students’ perception of academics from the friendliness by using: 1) The original campus climate data, and 2) the subset you created. Compare the two models in terms of the slope and R-squared and comment on any changes that happened. 
```{r}
199311
```

The slope is statistically significant in both models: 0.58 in the original dataset and slightly sleeper at 0.61 for the subset. The intercept also changes from 15.42 to 13.33 so both B_0 and B_1 are different in these linear models. However, the R^2 values show little difference in the approaches: although the R^2 is higher for the subset data, 26.44% is not much higher than 26.05%

### e)	Interpret the slope, intercepts, and R-squared for model resulting from the subset of the data you created within context 
```{r}
summary(newdata$friendlyenvp) #minimum in subset is not 0
```

For every unit increase of the perception of friendliness, we except the average of the perception of academic satisfaction to increase by 0.61 units. Since the minimum score for friendlyenvp is not 0, it makes no sense to interpret the intercept.  Finally, 26.44% of the variance in the perception of academic satisfication is explained by the perception of friendsliness on campus.

### f and g)	Conduct exploratory data analysis by creating the histograms, qqplot, plot of residuals vs. predictor. One quick way to perform exploratory data analysis is to use the common plot(name of the model) function. This will provide us with the majority of the plots we need. 
```{r}
hist(newdata$friendlyenvp, main = "Histogram: Perception of Friendliness")
boxplot(newdata$friendlyenvp)

qqnorm(newdata$friendlyenvp) 

plot(x=lm.new$residuals, y=newdata$friendlyenvp[!is.na(newdata$friendlyenvp)])   #residual vs predictor(friendly)
plot(x=lm.new$residuals, y=lm.new$fitted.values)   #residual vs predicted academic (y.hat)

newdata$friendlyenvp

par(mfrow=c(2,2))
plot(lm.new)
```



### h)	Draw the plot of friendlyenvironmentp^ (Y^) vs residual and vs. academicenvp. Are the different or the same? Explain why they are the same or different?

[fix question]
```{r}
##################################
#mistake in question???
plot(lm.new$fitted.values, lm.new$residuals) #y hat vs residual
plot( lm.new$residuals, friendlyenvp)

```
statement here


### i)	Conduct the ncv test to show that the principle of equality of error variance holds. 
```{r}
ncvTest(lm.new)
```
We have sufficient evidence to reject the null hypothesis
(that the variance is not constant?)

# Problem Two. 

### a)	In regression, conceptually speaking what do we mean by the principle of equality of error variance.

Conceptually, the equality of error variance means as the value of predicted y increases, the residuals will stay un-effected since there is no correlation between the residuals and the predicted y. 

### b)	In regression, mathematically what do we mean by the principle of least squares? 

In regression, the goal is to find the line that minimizes the amount of error between the actual and predicted values. Since the sum of errors will always equal 0 (because of positive and negative values), we look at the square of errors.  Therefore, the line with the smallest sum of residuals squared will be the best fit line. 

### c)	In regression plot of residuals vs. X or residuals vs. Y^ serve equally well for checking the principle of equality of error variance. Why is this the case? Explain conceptually and mathematically. 

x is multiplied by the estimated slope and added to the intercept to find the predicted values for y (or y^), therefore, since y^ is a function x, they serve the same purpose when checking for a correlation with residuals. 

### d)	Prove that sum of square of total = Sum of square of regression + sum of square of residual. (see answers to review exercise one) 

LATEX SOLUTION

### e)	Prove that slope and intercept result from placing the derivative of the sum of square of residuals equal to zero. 

PROOF HOW


#Problem Three 
Two researchers are studying the relationship between math and physics scores. Researcher A finds the covariance to be 700, Researchers B finds covariance to be 900. They both use a sample size of 100. 

### a)	Can we say that researcher B showed a stronger relationship between math and physics scores, yes or no and why? Make your point mathematically and conceptually. 

Both covariances show a strong positive relationship between math and physics scores. However, I would not say researcher B showed a stronger relationship because covariance is a unit measure and is not standardized. The researchers could be using different measures for their variables so we cannot compare covariances. We can however compare correlations.

MATH PROOF

### b)	Why do they call correlation standardized covariance?
Covariance indicates the direction of the relationship between variables. Correlation is the standardized covariance because it is divided by the standard deviation of each variable. This makes correlation unitless and able to compare the direction and strength of variable relationships. 

#Problem Four 

### a)	Using R, calculate the following, using the subset that you created. 
### b)
```{r}
#remove NA's for analysis
x <- friendlyenvp[!is.na(friendlyenvp)]
y <- academicenvp[!is.na(academicenvp)]

#set N
if(length(x) ==length(y)){
  N <- length(x)
}
N

#friendlyenvp summary stats
x.mean <- mean(x)
x.sd <- sd(x)
x.var <- var(x)

#academicenvp summary stats
y.mean <- mean(y)
y.sd <- sd(y)
y.var <- var(y)

a <- rbind(c(x.mean, x.sd, x.var),
      c(y.mean, y.sd, y.var))
row.names(a) <- c("friendlyenvp", "academicenvp")
colnames(a) <- c("mean", "sd", "variance")
a
```


		
### c)	Using R, calculate the coefficient of correlation and covariance between UCLA students’ perception of academics and friendliness of our environment at UCLA. 
```{r}
#covariance
covar <- (sum((x - x.mean)*(y - y.mean)))/(N-1)
covar
#check: covariance function
cov(x, y)

#correlation
corr <- covar / (x.sd*y.sd)
corr
#check: correlation function
cor(x, y)
```

Using what you reported in parts a and b, calculate… 
```{r}
k <- 1 #number of predictors
b1 <- covar / (x.sd^2) #slope
b0 <- y.mean - B1*x.mean #intercept
y.hat <- b0 + b1*x #predicted y
se <- sqrt((sum((y - y.hat)^2))/(N-2)) #sd of residuals

# TSS (Total Sum of Square) 
tss <- sum((y -y.mean)^2)
tss

# RSS (Residual Sum of Square)
rss <- sum((y - y.hat)^2)
rss

# SSR (Sum of Squares Regression)
ssr <- tss - rss
ssr

# SSX
ssx <- sum((x -x.mean)^2)
ssx

# Standard Error of the Slope
se.b <- se/sqrt(ssx)
se.b

# t-test of the slope (under the null B1 = 0)
t.val <- b1/se.b
t.val

# F-test of R-squared
F.val <- (ssr/k)/(rss/(N-k-1))
F.val

# Show that F = t^2
F.val == t.val^2
```
