\documentclass{article}
\usepackage[utf8]{inputenc}

\title{HW2}
\date{October 2020}

\begin{document}

\maketitle
\title{Problem 2}
\section{(a)}\textbf{Answers:} The equality of error variance means that as the number of predictors increases, the scatter of the residuals doesn't change.\\
\section{(b)}\textbf{Answers:} We are going to find a regression line such that the line minimize $e_1^2 + e^2_2 + e^2_3 + ... + e^2_n = \sum^N_{i=1}e^2_i$ where $e_i$'s are the residuals.

\section{(c)}\textbf{Answers:} Conceptually, $\hat{Y}$ is a linear function of $X$, when we talk about a relationship between residuals and $\hat{Y}$, we are actually talking about the relationship between residuals and $X$ in different scale and position. Mathematically, suppose $r = b_1\hat{Y} + b_0$ and $\hat{Y} = a_1X+a_0$, then $r = b_1a_1X+b_1a_0+b_0$.

\section{(d)}\textbf{Answers:} Sum of square of total = $\sum^N_{i=1} (Y_i - \bar{Y})^2 = \sum^N_{i=1} (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2$\\
$= \sum^N_{i=1}(Y_i - \hat{Y}_i)^2 + \sum^N_{i=1}(\hat{Y}_i - \bar{Y})^2 + \sum^N_{i=1} 2(Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y})$\\
$= SSE + SSR + \sum^N_{i=1}2e_i(\hat{Y}_i - \bar{Y})$\\
Since $\sum^N_{i=1} e_i = 0$, therefore Sum of square of total = SSE + SSR

\section{(e)}\textbf{Answers:} SSE = $\sum^N_{i=1} (Y_i - \hat{Y}_i)^2 = \sum^N_{i=1} (Y_i - b_1 - b_0X_i)^2$. We want to find $b_1$ and $b_0$ such that SSE is minimized. Let's take the derivative with respect to $b_1$ and $b_0$ for SSE. \\
Then we get $\frac{\partial SSE}{\partial b_1} = \sum^N_{i=1} -2(Y_i - b_1 - b_0X_i)$ and $\frac{\partial SSE}{\partial b_0} = \sum^N_{i=1} -2(Y_i - b_1 - b_0X_i)X_i$. \\
In order to minimize SSE, we set the first derivatives of SSE to 0. $\sum^N_{i=1} -2(Y_i - b_1 - b_0X_i) = 0$ and $\sum^N_{i=1} -2(Y_i - b_1 - b_0X_i)X_i = 0$. \\
For $b_1$ we got $\sum^N_{i=1} b_1 = \sum^N_{i=1} Y_i - \sum^N_{i=1} b_0X_i $\\
$Nb_1 = N\bar{Y} - N\bar{X}b_0$\\
$b_1 = \bar{Y} - \bar{X}b_0$
then we plug this $b_1$ into $\frac{\partial SSE}{\partial b_0}$, we got $\sum^N_{i=1} -2(Y_i - \bar{Y} + \bar{X}b_0 - b_0X_i)X_i = 0$\\
$\sum^N_{i=1} (Y_i - \bar{Y} + \bar{X}b_0 - b_0X_i) = 0$\\
$b_0 \sum^N_{i=1} (\bar{X} - X_i) = \sum^N_{i=1} (\bar{Y} - Y_i)$\\
$b_0 = \frac{\sum^N_{i=1} (\bar{Y} - Y_i)}{\sum^N_{i=1} (\bar{X} - X_i)}$\\
$= \frac{\frac{\sum_{i=1}^{N}\left(X_{i}-\bar{X}\right) *\left(Y_{i}-\bar{Y}\right)}{N-1}}{\frac{\sum_{i=1}^{N}\left(X_{i}-\bar{X}\right)^{2}}{N-1}}$\\
$ = \frac{S_{X Y}}{S_{X}^{2}}$\\

\title{Problem 3}

\textbf{(a) Answers:} Conceptually, corvariance can only give the growing trend of two scores. According to the corvariance, we can just say that the two scores are positive related, while we cannot conclude the strength between two scores. Mathematically, $S_{X Y}=\frac{\sum_{i=1}^{N}\left(X_{i}-\bar{X}\right) *\left(Y_{i}-\bar{Y}\right)}{N-1}$, according to the formula, we cannot see any information about standard deviation of each scores. If $\left(X_{i}-\bar{X}\right) >> 0$ and $\left(Y_{i}-\bar{Y}\right) >> 0$  , the corvariance can also be big, while the relation between two values can still be small. In such case, corvariance fails to show the strength of relations between two values.\\
\textbf{(b) Answers:} $r_{X Y}=\frac{\sum_{i=1}^{N}\left(X_{i}-\bar{X}\right) *\left(Y_{i}-\bar{Y}\right)}{S_{X} * S_{Y} * N-1}$. From the formula, we see that the coefficient of correlation takes the standard deviation of each scores into consideration by dividing the product of standard deviation of each score for the covariance.  


\end{document}
